++ id -u
+ myuid=0
++ id -g
+ mygid=0
+ set +e
++ getent passwd 0
+ uidentry=root:x:0:0:root:/root:/bin/ash
+ set -e
+ '[' -z root:x:0:0:root:/root:/bin/ash ']'
+ SPARK_K8S_CMD=driver
+ case "$SPARK_K8S_CMD" in
+ shift 1
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -n '' ']'
+ PYSPARK_ARGS=
+ '[' -n '' ']'
+ R_ARGS=
+ '[' -n '' ']'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ case "$SPARK_K8S_CMD" in
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /sbin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=192.168.28.37 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class com.amazonaws.eks.tpcds.DataGenTPCDS spark-internal s3a://spark-k8s-data/TPCDS-TEST-10T /opt/tpcds-kit/tools 10000 2000 false false true
19/09/08 01:44:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
DATA DIR is s3a://spark-k8s-data/TPCDS-TEST-10T
Tools dsdgen executable located in /opt/tpcds-kit/tools
Scale factor is 10000 GB
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/09/08 01:44:59 INFO SparkContext: Running Spark version 2.4.5-SNAPSHOT
19/09/08 01:44:59 INFO SparkContext: Submitted application: TPCDS DataGen 10000 GB
19/09/08 01:44:59 INFO SecurityManager: Changing view acls to: root
19/09/08 01:44:59 INFO SecurityManager: Changing modify acls to: root
19/09/08 01:44:59 INFO SecurityManager: Changing view acls groups to: 
19/09/08 01:44:59 INFO SecurityManager: Changing modify acls groups to: 
19/09/08 01:44:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/09/08 01:44:59 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
19/09/08 01:44:59 INFO SparkEnv: Registering MapOutputTracker
19/09/08 01:44:59 INFO SparkEnv: Registering BlockManagerMaster
19/09/08 01:44:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/09/08 01:44:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/09/08 01:44:59 INFO DiskBlockManager: Created local directory at /var/data/spark-a822e6c7-c58f-4ab8-817f-ed32b3b871ec/blockmgr-d022fd6b-5196-4c07-ae21-836b1e3b4c52
19/09/08 01:44:59 INFO MemoryStore: MemoryStore started with capacity 4.0 GB
19/09/08 01:44:59 INFO SparkEnv: Registering OutputCommitCoordinator
19/09/08 01:45:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/09/08 01:45:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://tpcds-datagen-10t-1567907094712-driver-svc.default.svc:4040
19/09/08 01:45:00 INFO SparkContext: Added JAR file:///opt/spark/examples/jars/eks-spark-examples-assembly-1.0.jar at spark://tpcds-datagen-10t-1567907094712-driver-svc.default.svc:7078/jars/eks-spark-examples-assembly-1.0.jar with timestamp 1567907100154
19/09/08 01:45:01 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
19/09/08 01:45:01 INFO NettyBlockTransferService: Server created on tpcds-datagen-10t-1567907094712-driver-svc.default.svc:7079
19/09/08 01:45:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/09/08 01:45:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, tpcds-datagen-10t-1567907094712-driver-svc.default.svc, 7079, None)
19/09/08 01:45:01 INFO BlockManagerMasterEndpoint: Registering block manager tpcds-datagen-10t-1567907094712-driver-svc.default.svc:7079 with 4.0 GB RAM, BlockManagerId(driver, tpcds-datagen-10t-1567907094712-driver-svc.default.svc, 7079, None)
19/09/08 01:45:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, tpcds-datagen-10t-1567907094712-driver-svc.default.svc, 7079, None)
19/09/08 01:45:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, tpcds-datagen-10t-1567907094712-driver-svc.default.svc, 7079, None)
19/09/08 01:45:03 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:04 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.60.168:60404) with ID 1
19/09/08 01:45:04 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.200:35370) with ID 3
19/09/08 01:45:04 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.44.111:58408) with ID 4
19/09/08 01:45:04 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.50.8:40948) with ID 5
19/09/08 01:45:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.18.200:45039 with 5.4 GB RAM, BlockManagerId(3, 192.168.18.200, 45039, None)
19/09/08 01:45:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.60.168:34647 with 5.4 GB RAM, BlockManagerId(1, 192.168.60.168, 34647, None)
19/09/08 01:45:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.44.111:36919 with 5.4 GB RAM, BlockManagerId(4, 192.168.44.111, 36919, None)
19/09/08 01:45:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.50.8:45107 with 5.4 GB RAM, BlockManagerId(5, 192.168.50.8, 45107, None)
19/09/08 01:45:04 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.38.66:42460) with ID 2
19/09/08 01:45:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.38.66:33095 with 5.4 GB RAM, BlockManagerId(2, 192.168.38.66, 33095, None)
19/09/08 01:45:06 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.23.44:51052) with ID 6
19/09/08 01:45:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.23.44:45019 with 5.4 GB RAM, BlockManagerId(6, 192.168.23.44, 45019, None)
19/09/08 01:45:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.31.109:44970) with ID 9
19/09/08 01:45:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.4.207:50254) with ID 8
19/09/08 01:45:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.51.198:57648) with ID 10
19/09/08 01:45:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.61.42:43964) with ID 7
19/09/08 01:45:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.61.42:42975 with 5.4 GB RAM, BlockManagerId(7, 192.168.61.42, 42975, None)
19/09/08 01:45:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.51.198:37265 with 5.4 GB RAM, BlockManagerId(10, 192.168.51.198, 37265, None)
19/09/08 01:45:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.4.207:46777 with 5.4 GB RAM, BlockManagerId(8, 192.168.4.207, 46777, None)
19/09/08 01:45:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.31.109:45829 with 5.4 GB RAM, BlockManagerId(9, 192.168.31.109, 45829, None)
19/09/08 01:45:08 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:09 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.62.132:50274) with ID 11
19/09/08 01:45:09 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.99:56428) with ID 12
19/09/08 01:45:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.62.132:34935 with 5.4 GB RAM, BlockManagerId(11, 192.168.62.132, 34935, None)
19/09/08 01:45:10 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.51.66:60412) with ID 14
19/09/08 01:45:10 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.25.84:52860) with ID 13
19/09/08 01:45:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.99:39543 with 5.4 GB RAM, BlockManagerId(12, 192.168.0.99, 39543, None)
19/09/08 01:45:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.51.66:42397 with 5.4 GB RAM, BlockManagerId(14, 192.168.51.66, 42397, None)
19/09/08 01:45:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.84:33231 with 5.4 GB RAM, BlockManagerId(13, 192.168.25.84, 33231, None)
19/09/08 01:45:10 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.37.252:48534) with ID 15
19/09/08 01:45:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.37.252:41147 with 5.4 GB RAM, BlockManagerId(15, 192.168.37.252, 41147, None)
19/09/08 01:45:12 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.9:50332) with ID 16
19/09/08 01:45:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.54:39980) with ID 19
19/09/08 01:45:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.5.126:40460) with ID 17
19/09/08 01:45:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.9:45933 with 5.4 GB RAM, BlockManagerId(16, 192.168.10.9, 45933, None)
19/09/08 01:45:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.18.54:46123 with 5.4 GB RAM, BlockManagerId(19, 192.168.18.54, 46123, None)
19/09/08 01:45:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.5.126:37405 with 5.4 GB RAM, BlockManagerId(17, 192.168.5.126, 37405, None)
19/09/08 01:45:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.49.225:57706) with ID 18
19/09/08 01:45:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.49.225:39285 with 5.4 GB RAM, BlockManagerId(18, 192.168.49.225, 39285, None)
19/09/08 01:45:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.53.62:53960) with ID 20
19/09/08 01:45:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.53.62:41351 with 5.4 GB RAM, BlockManagerId(20, 192.168.53.62, 41351, None)
19/09/08 01:45:15 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.22.62:59434) with ID 22
19/09/08 01:45:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.137:39122) with ID 21
19/09/08 01:45:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.4.129:58954) with ID 23
19/09/08 01:45:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.7.32:55126) with ID 25
19/09/08 01:45:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.22.62:42941 with 5.4 GB RAM, BlockManagerId(22, 192.168.22.62, 42941, None)
19/09/08 01:45:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.46.20:51618) with ID 24
19/09/08 01:45:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.12.137:45609 with 5.4 GB RAM, BlockManagerId(21, 192.168.12.137, 45609, None)
19/09/08 01:45:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.4.129:37917 with 5.4 GB RAM, BlockManagerId(23, 192.168.4.129, 37917, None)
19/09/08 01:45:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.7.32:35387 with 5.4 GB RAM, BlockManagerId(25, 192.168.7.32, 35387, None)
19/09/08 01:45:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.46.20:34701 with 5.4 GB RAM, BlockManagerId(24, 192.168.46.20, 34701, None)
19/09/08 01:45:17 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:18 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.61.233:39180) with ID 26
19/09/08 01:45:18 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.107:55740) with ID 29
19/09/08 01:45:18 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.44.29:57558) with ID 28
19/09/08 01:45:18 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.37.200:37932) with ID 30
19/09/08 01:45:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.61.233:43507 with 5.4 GB RAM, BlockManagerId(26, 192.168.61.233, 43507, None)
19/09/08 01:45:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.8.107:44281 with 5.4 GB RAM, BlockManagerId(29, 192.168.8.107, 44281, None)
19/09/08 01:45:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.44.29:39445 with 5.4 GB RAM, BlockManagerId(28, 192.168.44.29, 39445, None)
19/09/08 01:45:18 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.54.37:55808) with ID 27
19/09/08 01:45:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.37.200:35849 with 5.4 GB RAM, BlockManagerId(30, 192.168.37.200, 35849, None)
19/09/08 01:45:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.54.37:43759 with 5.4 GB RAM, BlockManagerId(27, 192.168.54.37, 43759, None)
19/09/08 01:45:20 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:20 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.4.64:35196) with ID 31
19/09/08 01:45:20 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.164:35866) with ID 33
19/09/08 01:45:20 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.97:53904) with ID 32
19/09/08 01:45:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.4.64:40611 with 5.4 GB RAM, BlockManagerId(31, 192.168.4.64, 40611, None)
19/09/08 01:45:20 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.54.132:48686) with ID 34
19/09/08 01:45:20 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.25.101:53006) with ID 35
19/09/08 01:45:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.12.97:41531 with 5.4 GB RAM, BlockManagerId(32, 192.168.12.97, 41531, None)
19/09/08 01:45:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.164:44445 with 5.4 GB RAM, BlockManagerId(33, 192.168.2.164, 44445, None)
19/09/08 01:45:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.54.132:34643 with 5.4 GB RAM, BlockManagerId(34, 192.168.54.132, 34643, None)
19/09/08 01:45:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.101:34395 with 5.4 GB RAM, BlockManagerId(35, 192.168.25.101, 34395, None)
19/09/08 01:45:22 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:23 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.53.129:52064) with ID 37
19/09/08 01:45:23 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.49.98:49380) with ID 38
19/09/08 01:45:23 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.60.161:41228) with ID 36
19/09/08 01:45:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.53.129:39811 with 5.4 GB RAM, BlockManagerId(37, 192.168.53.129, 39811, None)
19/09/08 01:45:23 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.65:39416) with ID 40
19/09/08 01:45:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.49.98:43037 with 5.4 GB RAM, BlockManagerId(38, 192.168.49.98, 43037, None)
19/09/08 01:45:24 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.58.158:57262) with ID 39
19/09/08 01:45:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.60.161:40969 with 5.4 GB RAM, BlockManagerId(36, 192.168.60.161, 40969, None)
19/09/08 01:45:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.18.65:38423 with 5.4 GB RAM, BlockManagerId(40, 192.168.18.65, 38423, None)
19/09/08 01:45:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.58.158:39223 with 5.4 GB RAM, BlockManagerId(39, 192.168.58.158, 39223, None)
19/09/08 01:45:25 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:25 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.5.161:57858) with ID 44
19/09/08 01:45:25 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.56.2:44252) with ID 42
19/09/08 01:45:26 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.55.117:33598) with ID 43
19/09/08 01:45:26 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.39.202:54908) with ID 41
19/09/08 01:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.5.161:38583 with 5.4 GB RAM, BlockManagerId(44, 192.168.5.161, 38583, None)
19/09/08 01:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.56.2:41765 with 5.4 GB RAM, BlockManagerId(42, 192.168.56.2, 41765, None)
19/09/08 01:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.55.117:40739 with 5.4 GB RAM, BlockManagerId(43, 192.168.55.117, 40739, None)
19/09/08 01:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.39.202:36373 with 5.4 GB RAM, BlockManagerId(41, 192.168.39.202, 36373, None)
19/09/08 01:45:26 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.37.143:60192) with ID 45
19/09/08 01:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.37.143:42395 with 5.4 GB RAM, BlockManagerId(45, 192.168.37.143, 42395, None)
19/09/08 01:45:28 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:29 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.27.133:45752) with ID 47
19/09/08 01:45:29 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.39.184:57722) with ID 46
19/09/08 01:45:29 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.32.158:53066) with ID 48
19/09/08 01:45:29 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.17.74:43184) with ID 49
19/09/08 01:45:29 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.59.7:40720) with ID 50
19/09/08 01:45:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.27.133:39661 with 5.4 GB RAM, BlockManagerId(47, 192.168.27.133, 39661, None)
19/09/08 01:45:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.39.184:42173 with 5.4 GB RAM, BlockManagerId(46, 192.168.39.184, 42173, None)
19/09/08 01:45:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.32.158:33117 with 5.4 GB RAM, BlockManagerId(48, 192.168.32.158, 33117, None)
19/09/08 01:45:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.17.74:37329 with 5.4 GB RAM, BlockManagerId(49, 192.168.17.74, 37329, None)
19/09/08 01:45:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.59.7:36347 with 5.4 GB RAM, BlockManagerId(50, 192.168.59.7, 36347, None)
19/09/08 01:45:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.27.209:55688) with ID 51
19/09/08 01:45:31 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/08 01:45:31 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
19/09/08 01:45:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.52.28:37680) with ID 52
19/09/08 01:45:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.47.182:35866) with ID 53
Only WARN
Generating TPCDS data
19/09/08 01:45:33 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
Generating table catalog_sales in database to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales with save mode Overwrite.
19/09/08 01:45:33 INFO TPCDSTables: Generating table catalog_sales in database to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales with save mode Overwrite.
19/09/08 01:45:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
19/09/08 01:50:55 WARN TaskSetManager: Lost task 25.0 in stage 0.0 (TID 25, 192.168.37.252, executor 15): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/_temporary/0/_temporary/attempt_20190908014538_0000_m_000025_25/part-00025-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet; isDirectory=false; length=514124604; replication=1; blocksize=33554432; modification_time=1567907232000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/part-00025-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 02:47:22 WARN TaskSetManager: Lost task 1037.0 in stage 0.0 (TID 1038, 192.168.4.129, executor 23): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/_temporary/0/_temporary/attempt_20190908014538_0000_m_001037_1038/part-01037-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet; isDirectory=false; length=514851996; replication=1; blocksize=33554432; modification_time=1567910592000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/part-01037-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 03:17:56 WARN TaskSetManager: Lost task 1586.0 in stage 0.0 (TID 1588, 192.168.49.225, executor 18): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/_temporary/0/_temporary/attempt_20190908014538_0000_m_001586_1588/part-01586-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet; isDirectory=false; length=514893546; replication=1; blocksize=33554432; modification_time=1567912449000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/part-01586-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 03:18:46 WARN TaskSetManager: Lost task 1606.0 in stage 0.0 (TID 1608, 192.168.53.162, executor 70): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/_temporary/0/_temporary/attempt_20190908014538_0000_m_001606_1608/part-01606-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet; isDirectory=false; length=513027755; replication=1; blocksize=33554432; modification_time=1567912533000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_sales/part-01606-10e4b037-4521-4ea1-80ea-c6a987fb8fc8-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table catalog_returns in database to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns with save mode Overwrite.
19/09/08 03:42:41 INFO TPCDSTables: Generating table catalog_returns in database to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns with save mode Overwrite.
19/09/08 03:44:21 WARN TaskSetManager: Lost task 273.0 in stage 1.0 (TID 2277, 192.168.18.65, executor 40): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns/_temporary/0/_temporary/attempt_20190908034244_0001_m_000273_2277/part-00273-045d4677-9d04-42cf-8b6f-5b34debc1c90-c000.snappy.parquet; isDirectory=false; length=50836329; replication=1; blocksize=33554432; modification_time=1567914261000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns/part-00273-045d4677-9d04-42cf-8b6f-5b34debc1c90-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 03:47:34 WARN TaskSetManager: Lost task 859.0 in stage 1.0 (TID 2864, 192.168.5.161, executor 44): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns/_temporary/0/_temporary/attempt_20190908034244_0001_m_000859_2864/part-00859-045d4677-9d04-42cf-8b6f-5b34debc1c90-c000.snappy.parquet; isDirectory=false; length=50923289; replication=1; blocksize=33554432; modification_time=1567914439000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns/part-00859-045d4677-9d04-42cf-8b6f-5b34debc1c90-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 03:51:34 WARN TaskSetManager: Lost task 1732.0 in stage 1.0 (TID 3738, 192.168.12.21, executor 76): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns/_temporary/0/_temporary/attempt_20190908034244_0001_m_001732_3738/part-01732-045d4677-9d04-42cf-8b6f-5b34debc1c90-c000.snappy.parquet; isDirectory=false; length=50714611; replication=1; blocksize=33554432; modification_time=1567914693000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_returns/part-01732-045d4677-9d04-42cf-8b6f-5b34debc1c90-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table inventory in database to s3a://spark-k8s-data/TPCDS-TEST-10T/inventory with save mode Overwrite.
19/09/08 03:53:01 INFO TPCDSTables: Generating table inventory in database to s3a://spark-k8s-data/TPCDS-TEST-10T/inventory with save mode Overwrite.
19/09/08 03:53:49 WARN TaskSetManager: Lost task 1005.0 in stage 2.0 (TID 5012, 192.168.37.252, executor 15): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/inventory/_temporary/0/_temporary/attempt_20190908035304_0002_m_001005_5012/part-01005-a5ef2941-1301-4cfe-aea5-e6d99bd00cb7-c000.snappy.parquet; isDirectory=false; length=3464173; replication=1; blocksize=33554432; modification_time=1567914829000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/inventory/part-01005-a5ef2941-1301-4cfe-aea5-e6d99bd00cb7-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 03:54:22 WARN TaskSetManager: Lost task 1768.0 in stage 2.0 (TID 5776, 192.168.10.107, executor 64): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/inventory/_temporary/0/_temporary/attempt_20190908035304_0002_m_001768_5776/part-01768-a5ef2941-1301-4cfe-aea5-e6d99bd00cb7-c000.snappy.parquet; isDirectory=false; length=3464093; replication=1; blocksize=33554432; modification_time=1567914862000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/inventory/part-01768-a5ef2941-1301-4cfe-aea5-e6d99bd00cb7-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table store_sales in database to s3a://spark-k8s-data/TPCDS-TEST-10T/store_sales with save mode Overwrite.
19/09/08 04:09:19 INFO TPCDSTables: Generating table store_sales in database to s3a://spark-k8s-data/TPCDS-TEST-10T/store_sales with save mode Overwrite.
19/09/08 05:22:50 WARN TaskSetManager: Lost task 885.0 in stage 3.0 (TID 6894, 192.168.51.139, executor 89): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/store_sales/_temporary/0/_temporary/attempt_20190908040924_0003_m_000885_6894/part-00885-ae6a661b-ed43-46e3-acd8-dd3e07ace147-c000.snappy.parquet; isDirectory=false; length=643300886; replication=1; blocksize=33554432; modification_time=1567919835000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/store_sales/part-00885-ae6a661b-ed43-46e3-acd8-dd3e07ace147-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 06:48:41 WARN TaskSetManager: Lost task 1989.0 in stage 3.0 (TID 7999, 192.168.53.129, executor 37): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/store_sales/_temporary/0/_temporary/attempt_20190908040924_0003_m_001989_7999/part-01989-ae6a661b-ed43-46e3-acd8-dd3e07ace147-c000.snappy.parquet; isDirectory=false; length=642661919; replication=1; blocksize=33554432; modification_time=1567925015000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/store_sales/part-01989-ae6a661b-ed43-46e3-acd8-dd3e07ace147-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table store_returns in database to s3a://spark-k8s-data/TPCDS-TEST-10T/store_returns with save mode Overwrite.
19/09/08 06:53:54 INFO TPCDSTables: Generating table store_returns in database to s3a://spark-k8s-data/TPCDS-TEST-10T/store_returns with save mode Overwrite.
19/09/08 06:57:10 WARN TaskSetManager: Lost task 377.0 in stage 4.0 (TID 8388, 192.168.54.37, executor 27): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/store_returns/_temporary/0/_temporary/attempt_20190908065357_0004_m_000377_8388/part-00377-c0cf1720-dbd1-4372-bbb7-a53937617656-c000.snappy.parquet; isDirectory=false; length=73621021; replication=1; blocksize=33554432; modification_time=1567925829000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/store_returns/part-00377-c0cf1720-dbd1-4372-bbb7-a53937617656-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table web_sales in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_sales with save mode Overwrite.
19/09/08 07:09:32 INFO TPCDSTables: Generating table web_sales in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_sales with save mode Overwrite.
19/09/08 07:12:11 WARN TaskSetManager: Lost task 26.0 in stage 5.0 (TID 10038, 192.168.32.158, executor 48): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/web_sales/_temporary/0/_temporary/attempt_20190908070935_0005_m_000026_10038/part-00026-3e69882a-694d-40e7-b30f-fb7d80c9acbc-c000.snappy.parquet; isDirectory=false; length=235839084; replication=1; blocksize=33554432; modification_time=1567926662000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/web_sales/part-00026-3e69882a-694d-40e7-b30f-fb7d80c9acbc-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table web_returns in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns with save mode Overwrite.
19/09/08 08:06:25 INFO TPCDSTables: Generating table web_returns in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns with save mode Overwrite.
19/09/08 08:06:45 WARN TaskSetManager: Lost task 43.0 in stage 6.0 (TID 12056, 192.168.29.119, executor 69): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/_temporary/0/_temporary/attempt_20190908080628_0006_m_000043_12056/part-00043-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet; isDirectory=false; length=25667975; replication=1; blocksize=33554432; modification_time=1567930005000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/part-00043-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 08:09:17 WARN TaskSetManager: Lost task 1105.0 in stage 6.0 (TID 13119, 192.168.12.106, executor 78): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/_temporary/0/_temporary/attempt_20190908080628_0006_m_001105_13119/part-01105-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet; isDirectory=false; length=25765631; replication=1; blocksize=33554432; modification_time=1567930156000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/part-01105-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 08:09:53 WARN TaskSetManager: Lost task 1314.0 in stage 6.0 (TID 13330, 192.168.5.29, executor 84): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/_temporary/0/_temporary/attempt_20190908080628_0006_m_001314_13330/part-01314-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet; isDirectory=false; length=25619894; replication=1; blocksize=33554432; modification_time=1567930193000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/part-01314-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

19/09/08 08:10:32 WARN TaskSetManager: Lost task 1591.0 in stage 6.0 (TID 13608, 192.168.27.133, executor 47): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/_temporary/0/_temporary/attempt_20190908080628_0006_m_001591_13608/part-01591-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet; isDirectory=false; length=25726989; replication=1; blocksize=33554432; modification_time=1567930232000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/web_returns/part-01591-579879e4-e135-44bc-9b5d-f0491d3a5e32-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table call_center in database to s3a://spark-k8s-data/TPCDS-TEST-10T/call_center with save mode Overwrite.
19/09/08 08:11:37 INFO TPCDSTables: Generating table call_center in database to s3a://spark-k8s-data/TPCDS-TEST-10T/call_center with save mode Overwrite.
Generating table catalog_page in database to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_page with save mode Overwrite.
19/09/08 08:11:39 INFO TPCDSTables: Generating table catalog_page in database to s3a://spark-k8s-data/TPCDS-TEST-10T/catalog_page with save mode Overwrite.
19/09/08 08:11:42 INFO TPCDSTables: Generating table customer in database to s3a://spark-k8s-data/TPCDS-TEST-10T/customer with save mode Overwrite.
Generating table customer in database to s3a://spark-k8s-data/TPCDS-TEST-10T/customer with save mode Overwrite.
Generating table customer_address in database to s3a://spark-k8s-data/TPCDS-TEST-10T/customer_address with save mode Overwrite.
19/09/08 08:12:26 INFO TPCDSTables: Generating table customer_address in database to s3a://spark-k8s-data/TPCDS-TEST-10T/customer_address with save mode Overwrite.
19/09/08 08:12:53 WARN TaskSetManager: Lost task 1713.0 in stage 10.0 (TID 21731, 192.168.18.200, executor 3): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-10T/customer_address/_temporary/0/_temporary/attempt_20190908081228_0010_m_001713_21731/part-01713-58cf115e-65e9-407d-9db9-6653f368e664-c000.snappy.parquet; isDirectory=false; length=383281; replication=1; blocksize=33554432; modification_time=1567930373000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-10T/customer_address/part-01713-58cf115e-65e9-407d-9db9-6653f368e664-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table customer_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-10T/customer_demographics with save mode Overwrite.
19/09/08 08:13:03 INFO TPCDSTables: Generating table customer_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-10T/customer_demographics with save mode Overwrite.
Generating table date_dim in database to s3a://spark-k8s-data/TPCDS-TEST-10T/date_dim with save mode Overwrite.
19/09/08 08:13:32 INFO TPCDSTables: Generating table date_dim in database to s3a://spark-k8s-data/TPCDS-TEST-10T/date_dim with save mode Overwrite.
Generating table household_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-10T/household_demographics with save mode Overwrite.
19/09/08 08:13:36 INFO TPCDSTables: Generating table household_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-10T/household_demographics with save mode Overwrite.
Generating table income_band in database to s3a://spark-k8s-data/TPCDS-TEST-10T/income_band with save mode Overwrite.
19/09/08 08:13:38 INFO TPCDSTables: Generating table income_band in database to s3a://spark-k8s-data/TPCDS-TEST-10T/income_band with save mode Overwrite.
Generating table item in database to s3a://spark-k8s-data/TPCDS-TEST-10T/item with save mode Overwrite.
19/09/08 08:13:40 INFO TPCDSTables: Generating table item in database to s3a://spark-k8s-data/TPCDS-TEST-10T/item with save mode Overwrite.
Generating table promotion in database to s3a://spark-k8s-data/TPCDS-TEST-10T/promotion with save mode Overwrite.
19/09/08 08:13:51 INFO TPCDSTables: Generating table promotion in database to s3a://spark-k8s-data/TPCDS-TEST-10T/promotion with save mode Overwrite.
Generating table reason in database to s3a://spark-k8s-data/TPCDS-TEST-10T/reason with save mode Overwrite.
19/09/08 08:13:53 INFO TPCDSTables: Generating table reason in database to s3a://spark-k8s-data/TPCDS-TEST-10T/reason with save mode Overwrite.
Generating table ship_mode in database to s3a://spark-k8s-data/TPCDS-TEST-10T/ship_mode with save mode Overwrite.
19/09/08 08:13:56 INFO TPCDSTables: Generating table ship_mode in database to s3a://spark-k8s-data/TPCDS-TEST-10T/ship_mode with save mode Overwrite.
Generating table store in database to s3a://spark-k8s-data/TPCDS-TEST-10T/store with save mode Overwrite.
19/09/08 08:13:58 INFO TPCDSTables: Generating table store in database to s3a://spark-k8s-data/TPCDS-TEST-10T/store with save mode Overwrite.
Generating table time_dim in database to s3a://spark-k8s-data/TPCDS-TEST-10T/time_dim with save mode Overwrite.
19/09/08 08:14:00 INFO TPCDSTables: Generating table time_dim in database to s3a://spark-k8s-data/TPCDS-TEST-10T/time_dim with save mode Overwrite.
Generating table warehouse in database to s3a://spark-k8s-data/TPCDS-TEST-10T/warehouse with save mode Overwrite.
19/09/08 08:14:03 INFO TPCDSTables: Generating table warehouse in database to s3a://spark-k8s-data/TPCDS-TEST-10T/warehouse with save mode Overwrite.
19/09/08 08:14:05 INFO TPCDSTables: Generating table web_page in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_page with save mode Overwrite.
Generating table web_page in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_page with save mode Overwrite.
Generating table web_site in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_site with save mode Overwrite.
19/09/08 08:14:08 INFO TPCDSTables: Generating table web_site in database to s3a://spark-k8s-data/TPCDS-TEST-10T/web_site with save mode Overwrite.
Data generated at s3a://spark-k8s-data/TPCDS-TEST-10T
19/09/08 08:14:11 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
