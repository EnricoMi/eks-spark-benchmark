++ id -u
+ myuid=0
++ id -g
+ mygid=0
+ set +e
++ getent passwd 0
+ uidentry=root:x:0:0:root:/root:/bin/bash
+ set -e
+ '[' -z root:x:0:0:root:/root:/bin/bash ']'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ '[' -z ']'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=192.168.10.21 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class com.amazonaws.eks.tpcds.DataGenTPCDS spark-internal s3a://spark-k8s-data/TPCDS-TEST-100G /opt/tpcds-kit/tools 100 200 false false true
19/11/07 23:00:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
DATA DIR is s3a://spark-k8s-data/TPCDS-TEST-100G
Tools dsdgen executable located in /opt/tpcds-kit/tools
Scale factor is 100 GB
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/11/07 23:00:07 INFO SparkContext: Running Spark version 3.0.0-SNAPSHOT
19/11/07 23:00:07 INFO ResourceUtils: ==============================================================
19/11/07 23:00:07 INFO ResourceUtils: Resources for spark.driver:

19/11/07 23:00:07 INFO ResourceUtils: ==============================================================
19/11/07 23:00:07 INFO SparkContext: Submitted application: TPCDS DataGen 100 GB
19/11/07 23:00:07 INFO SecurityManager: Changing view acls to: root
19/11/07 23:00:07 INFO SecurityManager: Changing modify acls to: root
19/11/07 23:00:07 INFO SecurityManager: Changing view acls groups to:
19/11/07 23:00:07 INFO SecurityManager: Changing modify acls groups to:
19/11/07 23:00:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/11/07 23:00:07 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
19/11/07 23:00:07 INFO SparkEnv: Registering MapOutputTracker
19/11/07 23:00:07 INFO SparkEnv: Registering BlockManagerMaster
19/11/07 23:00:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/11/07 23:00:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/11/07 23:00:07 INFO DiskBlockManager: Created local directory at /var/data/spark-ea5a098d-f2bf-422b-ad96-707c12d32029/blockmgr-e8b2bc10-d5b4-4c53-8d21-a37b196e5eef
19/11/07 23:00:07 INFO MemoryStore: MemoryStore started with capacity 1953.6 MiB
19/11/07 23:00:07 INFO SparkEnv: Registering OutputCommitCoordinator
19/11/07 23:00:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/11/07 23:00:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://tpcds-datagen-100g-1573167602682-driver-svc.default.svc:4040
19/11/07 23:00:08 INFO SparkContext: Added JAR file:///opt/spark/examples/jars/eks-spark-examples-assembly-1.0.jar at spark://tpcds-datagen-100g-1573167602682-driver-svc.default.svc:7078/jars/eks-spark-examples-assembly-1.0.jar with timestamp 1573167608268
19/11/07 23:00:08 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
19/11/07 23:00:09 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/11/07 23:00:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
19/11/07 23:00:09 INFO NettyBlockTransferService: Server created on tpcds-datagen-100g-1573167602682-driver-svc.default.svc:7079
19/11/07 23:00:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/11/07 23:00:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, tpcds-datagen-100g-1573167602682-driver-svc.default.svc, 7079, None)
19/11/07 23:00:09 INFO BlockManagerMasterEndpoint: Registering block manager tpcds-datagen-100g-1573167602682-driver-svc.default.svc:7079 with 1953.6 MiB RAM, BlockManagerId(driver, tpcds-datagen-100g-1573167602682-driver-svc.default.svc, 7079, None)
19/11/07 23:00:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, tpcds-datagen-100g-1573167602682-driver-svc.default.svc, 7079, None)
19/11/07 23:00:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, tpcds-datagen-100g-1573167602682-driver-svc.default.svc, 7079, None)
19/11/07 23:00:12 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/11/07 23:00:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.22.148:54554) with ID 5
19/11/07 23:00:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.52.235:57018) with ID 1
19/11/07 23:00:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.57.94:44074) with ID 2
19/11/07 23:00:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.17.232:58508) with ID 3
19/11/07 23:00:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.57.94:32973 with 4.3 GiB RAM, BlockManagerId(2, 192.168.57.94, 32973, None)
19/11/07 23:00:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.17.232:37683 with 4.3 GiB RAM, BlockManagerId(3, 192.168.17.232, 37683, None)
19/11/07 23:00:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.52.235:34151 with 4.3 GiB RAM, BlockManagerId(1, 192.168.52.235, 34151, None)
19/11/07 23:00:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.22.148:38567 with 4.3 GiB RAM, BlockManagerId(5, 192.168.22.148, 38567, None)
19/11/07 23:00:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.48.151:47130) with ID 4
19/11/07 23:00:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.48.151:39173 with 4.3 GiB RAM, BlockManagerId(4, 192.168.48.151, 39173, None)
19/11/07 23:00:15 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/11/07 23:00:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.27.107:53258) with ID 6
19/11/07 23:00:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.50.216:51784) with ID 8
19/11/07 23:00:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.6.46:36408) with ID 7
19/11/07 23:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.27.107:34941 with 4.3 GiB RAM, BlockManagerId(6, 192.168.27.107, 34941, None)
19/11/07 23:00:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.47.167:51602) with ID 9
19/11/07 23:00:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.46.222:57764) with ID 10
19/11/07 23:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.6.46:33355 with 4.3 GiB RAM, BlockManagerId(7, 192.168.6.46, 33355, None)
19/11/07 23:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.50.216:37653 with 4.3 GiB RAM, BlockManagerId(8, 192.168.50.216, 37653, None)
19/11/07 23:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.47.167:40075 with 4.3 GiB RAM, BlockManagerId(9, 192.168.47.167, 40075, None)
19/11/07 23:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.46.222:43981 with 4.3 GiB RAM, BlockManagerId(10, 192.168.46.222, 43981, None)
19/11/07 23:00:18 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/11/07 23:00:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.5.125:54362) with ID 11
19/11/07 23:00:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.46.5:34058) with ID 15
19/11/07 23:00:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.7.40:49520) with ID 12
19/11/07 23:00:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.206:37870) with ID 14
19/11/07 23:00:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.5.125:38117 with 4.3 GiB RAM, BlockManagerId(11, 192.168.5.125, 38117, None)
19/11/07 23:00:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.37.108:41950) with ID 13
19/11/07 23:00:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.46.5:40623 with 4.3 GiB RAM, BlockManagerId(15, 192.168.46.5, 40623, None)
19/11/07 23:00:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.7.40:44905 with 4.3 GiB RAM, BlockManagerId(12, 192.168.7.40, 44905, None)
19/11/07 23:00:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.18.206:32831 with 4.3 GiB RAM, BlockManagerId(14, 192.168.18.206, 32831, None)
19/11/07 23:00:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.37.108:44711 with 4.3 GiB RAM, BlockManagerId(13, 192.168.37.108, 44711, None)
19/11/07 23:00:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.22.136:45476) with ID 16
19/11/07 23:00:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.51.185:48114) with ID 20
19/11/07 23:00:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.22.136:38931 with 4.3 GiB RAM, BlockManagerId(16, 192.168.22.136, 38931, None)
19/11/07 23:00:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.58.148:38100) with ID 17
19/11/07 23:00:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.51.185:45621 with 4.3 GiB RAM, BlockManagerId(20, 192.168.51.185, 45621, None)
19/11/07 23:00:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.58.148:40315 with 4.3 GiB RAM, BlockManagerId(17, 192.168.58.148, 40315, None)
19/11/07 23:00:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.46.225:43358) with ID 19
19/11/07 23:00:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.46.225:37617 with 4.3 GiB RAM, BlockManagerId(19, 192.168.46.225, 37617, None)
19/11/07 23:00:39 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
Only WARN
Generating TPCDS data
19/11/07 23:00:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
Generating table catalog_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_sales with save mode Overwrite.
19/11/07 23:00:42 INFO TPCDSTables: Generating table catalog_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_sales with save mode Overwrite.
19/11/07 23:00:42 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
Generating table catalog_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_returns with save mode Overwrite.
19/11/07 23:03:13 INFO TPCDSTables: Generating table catalog_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_returns with save mode Overwrite.
Generating table inventory in database to s3a://spark-k8s-data/TPCDS-TEST-100G/inventory with save mode Overwrite.
19/11/07 23:03:36 INFO TPCDSTables: Generating table inventory in database to s3a://spark-k8s-data/TPCDS-TEST-100G/inventory with save mode Overwrite.
Generating table store_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_sales with save mode Overwrite.
19/11/07 23:04:21 INFO TPCDSTables: Generating table store_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_sales with save mode Overwrite.
Generating table store_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_returns with save mode Overwrite.
19/11/07 23:07:20 INFO TPCDSTables: Generating table store_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_returns with save mode Overwrite.
Generating table web_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_sales with save mode Overwrite.
19/11/07 23:07:47 INFO TPCDSTables: Generating table web_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_sales with save mode Overwrite.
Generating table web_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_returns with save mode Overwrite.
19/11/07 23:09:18 INFO TPCDSTables: Generating table web_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_returns with save mode Overwrite.
Generating table call_center in database to s3a://spark-k8s-data/TPCDS-TEST-100G/call_center with save mode Overwrite.
19/11/07 23:09:35 INFO TPCDSTables: Generating table call_center in database to s3a://spark-k8s-data/TPCDS-TEST-100G/call_center with save mode Overwrite.
Generating table catalog_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_page with save mode Overwrite.
19/11/07 23:09:44 INFO TPCDSTables: Generating table catalog_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_page with save mode Overwrite.
Generating table customer in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer with save mode Overwrite.
19/11/07 23:09:53 INFO TPCDSTables: Generating table customer in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer with save mode Overwrite.
Generating table customer_address in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_address with save mode Overwrite.
19/11/07 23:10:12 INFO TPCDSTables: Generating table customer_address in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_address with save mode Overwrite.
Generating table customer_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_demographics with save mode Overwrite.
19/11/07 23:11:19 INFO TPCDSTables: Generating table customer_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_demographics with save mode Overwrite.
Generating table date_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/date_dim with save mode Overwrite.
19/11/07 23:12:48 INFO TPCDSTables: Generating table date_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/date_dim with save mode Overwrite.
Generating table household_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/household_demographics with save mode Overwrite.
19/11/07 23:13:33 INFO TPCDSTables: Generating table household_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/household_demographics with save mode Overwrite.
Generating table income_band in database to s3a://spark-k8s-data/TPCDS-TEST-100G/income_band with save mode Overwrite.
19/11/07 23:14:16 INFO TPCDSTables: Generating table income_band in database to s3a://spark-k8s-data/TPCDS-TEST-100G/income_band with save mode Overwrite.
Generating table item in database to s3a://spark-k8s-data/TPCDS-TEST-100G/item with save mode Overwrite.
19/11/07 23:15:06 INFO TPCDSTables: Generating table item in database to s3a://spark-k8s-data/TPCDS-TEST-100G/item with save mode Overwrite.
Generating table promotion in database to s3a://spark-k8s-data/TPCDS-TEST-100G/promotion with save mode Overwrite.
19/11/07 23:15:47 INFO TPCDSTables: Generating table promotion in database to s3a://spark-k8s-data/TPCDS-TEST-100G/promotion with save mode Overwrite.
Generating table reason in database to s3a://spark-k8s-data/TPCDS-TEST-100G/reason with save mode Overwrite.
19/11/07 23:16:26 INFO TPCDSTables: Generating table reason in database to s3a://spark-k8s-data/TPCDS-TEST-100G/reason with save mode Overwrite.
Generating table ship_mode in database to s3a://spark-k8s-data/TPCDS-TEST-100G/ship_mode with save mode Overwrite.
19/11/07 23:17:17 INFO TPCDSTables: Generating table ship_mode in database to s3a://spark-k8s-data/TPCDS-TEST-100G/ship_mode with save mode Overwrite.
Generating table store in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store with save mode Overwrite.
19/11/07 23:17:51 INFO TPCDSTables: Generating table store in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store with save mode Overwrite.
Generating table time_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/time_dim with save mode Overwrite.
19/11/07 23:18:37 INFO TPCDSTables: Generating table time_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/time_dim with save mode Overwrite.
Generating table warehouse in database to s3a://spark-k8s-data/TPCDS-TEST-100G/warehouse with save mode Overwrite.
19/11/07 23:19:20 INFO TPCDSTables: Generating table warehouse in database to s3a://spark-k8s-data/TPCDS-TEST-100G/warehouse with save mode Overwrite.
Generating table web_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_page with save mode Overwrite.
19/11/07 23:20:09 INFO TPCDSTables: Generating table web_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_page with save mode Overwrite.
Generating table web_site in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_site with save mode Overwrite.
19/11/07 23:20:51 INFO TPCDSTables: Generating table web_site in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_site with save mode Overwrite.
Data generated at s3a://spark-k8s-data/TPCDS-TEST-100G
19/11/07 23:21:36 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
