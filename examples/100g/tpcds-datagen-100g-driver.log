++ id -u
+ myuid=0
++ id -g
+ mygid=0
+ set +e
++ getent passwd 0
+ uidentry=root:x:0:0:root:/root:/bin/ash
+ set -e
+ '[' -z root:x:0:0:root:/root:/bin/ash ']'
+ SPARK_K8S_CMD=driver
+ case "$SPARK_K8S_CMD" in
+ shift 1
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -n '' ']'
+ PYSPARK_ARGS=
+ '[' -n '' ']'
+ R_ARGS=
+ '[' -n '' ']'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ case "$SPARK_K8S_CMD" in
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /sbin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=192.168.29.3 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class com.amazonaws.eks.tpcds.DataGenTPCDS spark-internal s3a://spark-k8s-data/TPCDS-TEST-100G /opt/tpcds-kit/tools 100 100 false false true
19/09/07 23:30:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
DATA DIR is s3a://spark-k8s-data/TPCDS-TEST-100G
Tools dsdgen executable located in /opt/tpcds-kit/tools
Scale factor is 100 GB
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/09/07 23:30:11 INFO SparkContext: Running Spark version 2.4.5-SNAPSHOT
19/09/07 23:30:12 INFO SparkContext: Submitted application: TPCDS DataGen 100 GB
19/09/07 23:30:12 INFO SecurityManager: Changing view acls to: root
19/09/07 23:30:12 INFO SecurityManager: Changing modify acls to: root
19/09/07 23:30:12 INFO SecurityManager: Changing view acls groups to: 
19/09/07 23:30:12 INFO SecurityManager: Changing modify acls groups to: 
19/09/07 23:30:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/09/07 23:30:12 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
19/09/07 23:30:12 INFO SparkEnv: Registering MapOutputTracker
19/09/07 23:30:12 INFO SparkEnv: Registering BlockManagerMaster
19/09/07 23:30:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/09/07 23:30:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/09/07 23:30:12 INFO DiskBlockManager: Created local directory at /var/data/spark-74506677-d5a2-4dad-85a2-2bb818864a4a/blockmgr-377d3917-ee18-43eb-8697-b8ac83b25202
19/09/07 23:30:12 INFO MemoryStore: MemoryStore started with capacity 4.0 GB
19/09/07 23:30:12 INFO SparkEnv: Registering OutputCommitCoordinator
19/09/07 23:30:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/09/07 23:30:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://tpcds-datagen-100g-1567899007231-driver-svc.default.svc:4040
19/09/07 23:30:12 INFO SparkContext: Added JAR file:///opt/spark/examples/jars/eks-spark-examples-assembly-1.0.jar at spark://tpcds-datagen-100g-1567899007231-driver-svc.default.svc:7078/jars/eks-spark-examples-assembly-1.0.jar with timestamp 1567899012704
19/09/07 23:30:13 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
19/09/07 23:30:13 INFO NettyBlockTransferService: Server created on tpcds-datagen-100g-1567899007231-driver-svc.default.svc:7079
19/09/07 23:30:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/09/07 23:30:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, tpcds-datagen-100g-1567899007231-driver-svc.default.svc, 7079, None)
19/09/07 23:30:13 INFO BlockManagerMasterEndpoint: Registering block manager tpcds-datagen-100g-1567899007231-driver-svc.default.svc:7079 with 4.0 GB RAM, BlockManagerId(driver, tpcds-datagen-100g-1567899007231-driver-svc.default.svc, 7079, None)
19/09/07 23:30:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, tpcds-datagen-100g-1567899007231-driver-svc.default.svc, 7079, None)
19/09/07 23:30:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, tpcds-datagen-100g-1567899007231-driver-svc.default.svc, 7079, None)
19/09/07 23:30:16 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:17 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.59.147:51914) with ID 3
19/09/07 23:30:17 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.59.194:54140) with ID 1
19/09/07 23:30:17 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.65:46234) with ID 2
19/09/07 23:30:17 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.13.5:57878) with ID 5
19/09/07 23:30:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.59.147:40603 with 4.3 GB RAM, BlockManagerId(3, 192.168.59.147, 40603, None)
19/09/07 23:30:17 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.25.25:45282) with ID 4
19/09/07 23:30:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.59.194:37065 with 4.3 GB RAM, BlockManagerId(1, 192.168.59.194, 37065, None)
19/09/07 23:30:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.11.65:36031 with 4.3 GB RAM, BlockManagerId(2, 192.168.11.65, 36031, None)
19/09/07 23:30:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.13.5:40803 with 4.3 GB RAM, BlockManagerId(5, 192.168.13.5, 40803, None)
19/09/07 23:30:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.25:43953 with 4.3 GB RAM, BlockManagerId(4, 192.168.25.25, 43953, None)
19/09/07 23:30:19 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.27.158:48348) with ID 9
19/09/07 23:30:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.35.237:45626) with ID 6
19/09/07 23:30:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.3.142:54520) with ID 8
19/09/07 23:30:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.45.171:44908) with ID 7
19/09/07 23:30:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.35.237:45135 with 4.3 GB RAM, BlockManagerId(6, 192.168.35.237, 45135, None)
19/09/07 23:30:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.27.158:40155 with 4.3 GB RAM, BlockManagerId(9, 192.168.27.158, 40155, None)
19/09/07 23:30:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.3.142:33161 with 4.3 GB RAM, BlockManagerId(8, 192.168.3.142, 33161, None)
19/09/07 23:30:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.45.171:42325 with 4.3 GB RAM, BlockManagerId(7, 192.168.45.171, 42325, None)
19/09/07 23:30:19 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.54.132:37262) with ID 10
19/09/07 23:30:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.54.132:39687 with 4.3 GB RAM, BlockManagerId(10, 192.168.54.132, 39687, None)
19/09/07 23:30:21 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.23.206:39354) with ID 11
19/09/07 23:30:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.56.202:56828) with ID 12
19/09/07 23:30:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.39.49:35730) with ID 13
19/09/07 23:30:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.23.206:43173 with 4.3 GB RAM, BlockManagerId(11, 192.168.23.206, 43173, None)
19/09/07 23:30:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.39.49:36345 with 4.3 GB RAM, BlockManagerId(13, 192.168.39.49, 36345, None)
19/09/07 23:30:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.56.202:34597 with 4.3 GB RAM, BlockManagerId(12, 192.168.56.202, 34597, None)
19/09/07 23:30:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.60.168:46348) with ID 15
19/09/07 23:30:22 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.47.158:36494) with ID 14
19/09/07 23:30:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.60.168:37159 with 4.3 GB RAM, BlockManagerId(15, 192.168.60.168, 37159, None)
19/09/07 23:30:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.47.158:33197 with 4.3 GB RAM, BlockManagerId(14, 192.168.47.158, 33197, None)
19/09/07 23:30:24 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.3.201:33460) with ID 16
19/09/07 23:30:24 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:24 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.43.229:58938) with ID 18
19/09/07 23:30:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.3.201:35747 with 4.3 GB RAM, BlockManagerId(16, 192.168.3.201, 35747, None)
19/09/07 23:30:24 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.107:41850) with ID 17
19/09/07 23:30:24 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.204:51626) with ID 19
19/09/07 23:30:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.229:38049 with 4.3 GB RAM, BlockManagerId(18, 192.168.43.229, 38049, None)
19/09/07 23:30:25 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.23.123:59884) with ID 20
19/09/07 23:30:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.8.107:46521 with 4.3 GB RAM, BlockManagerId(17, 192.168.8.107, 46521, None)
19/09/07 23:30:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.8.204:40725 with 4.3 GB RAM, BlockManagerId(19, 192.168.8.204, 40725, None)
19/09/07 23:30:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.23.123:37977 with 4.3 GB RAM, BlockManagerId(20, 192.168.23.123, 37977, None)
19/09/07 23:30:27 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:27 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.63.239:55040) with ID 21
19/09/07 23:30:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.63.239:42061 with 4.3 GB RAM, BlockManagerId(21, 192.168.63.239, 42061, None)
19/09/07 23:30:28 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.6.59:36644) with ID 22
19/09/07 23:30:28 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.31.109:53308) with ID 25
19/09/07 23:30:28 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.51.139:53060) with ID 24
19/09/07 23:30:28 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.58.158:58998) with ID 23
19/09/07 23:30:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.6.59:44591 with 4.3 GB RAM, BlockManagerId(22, 192.168.6.59, 44591, None)
19/09/07 23:30:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.31.109:42393 with 4.3 GB RAM, BlockManagerId(25, 192.168.31.109, 42393, None)
19/09/07 23:30:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.51.139:43119 with 4.3 GB RAM, BlockManagerId(24, 192.168.51.139, 43119, None)
19/09/07 23:30:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.58.158:39893 with 4.3 GB RAM, BlockManagerId(23, 192.168.58.158, 39893, None)
19/09/07 23:30:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.6.66:40340) with ID 26
19/09/07 23:30:31 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.6.66:35817 with 4.3 GB RAM, BlockManagerId(26, 192.168.6.66, 35817, None)
19/09/07 23:30:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.50.233:54452) with ID 28
19/09/07 23:30:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.21.197:60384) with ID 29
19/09/07 23:30:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.50.233:38681 with 4.3 GB RAM, BlockManagerId(28, 192.168.50.233, 38681, None)
19/09/07 23:30:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.59.81:45556) with ID 30
19/09/07 23:30:31 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.37.143:42226) with ID 27
19/09/07 23:30:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.21.197:35329 with 4.3 GB RAM, BlockManagerId(29, 192.168.21.197, 35329, None)
19/09/07 23:30:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.59.81:32797 with 4.3 GB RAM, BlockManagerId(30, 192.168.59.81, 32797, None)
19/09/07 23:30:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.37.143:46181 with 4.3 GB RAM, BlockManagerId(27, 192.168.37.143, 46181, None)
19/09/07 23:30:34 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.56.194:59840) with ID 32
19/09/07 23:30:34 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:34 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.5.161:46820) with ID 31
19/09/07 23:30:34 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.47:42684) with ID 33
19/09/07 23:30:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.56.194:41185 with 4.3 GB RAM, BlockManagerId(32, 192.168.56.194, 41185, None)
19/09/07 23:30:34 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.17.202:52978) with ID 34
19/09/07 23:30:34 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.61.233:33766) with ID 35
19/09/07 23:30:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.5.161:44485 with 4.3 GB RAM, BlockManagerId(31, 192.168.5.161, 44485, None)
19/09/07 23:30:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.47:43985 with 4.3 GB RAM, BlockManagerId(33, 192.168.2.47, 43985, None)
19/09/07 23:30:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.17.202:35727 with 4.3 GB RAM, BlockManagerId(34, 192.168.17.202, 35727, None)
19/09/07 23:30:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.61.233:40587 with 4.3 GB RAM, BlockManagerId(35, 192.168.61.233, 40587, None)
19/09/07 23:30:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.46.52:38796) with ID 36
19/09/07 23:30:37 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/09/07 23:30:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.106:60258) with ID 39
19/09/07 23:30:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.37.252:52448) with ID 37
19/09/07 23:30:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.17.121:48540) with ID 38
19/09/07 23:30:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.46.52:37251 with 4.3 GB RAM, BlockManagerId(36, 192.168.46.52, 37251, None)
19/09/07 23:30:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.19.67:37162) with ID 40
19/09/07 23:30:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.12.106:34459 with 4.3 GB RAM, BlockManagerId(39, 192.168.12.106, 34459, None)
19/09/07 23:30:37 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/09/07 23:30:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.37.252:42651 with 4.3 GB RAM, BlockManagerId(37, 192.168.37.252, 42651, None)
19/09/07 23:30:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.17.121:35173 with 4.3 GB RAM, BlockManagerId(38, 192.168.17.121, 35173, None)
Only WARN
Generating TPCDS data
19/09/07 23:30:40 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
Generating table catalog_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_sales with save mode Overwrite.
19/09/07 23:30:40 INFO TPCDSTables: Generating table catalog_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_sales with save mode Overwrite.
19/09/07 23:30:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
Generating table catalog_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_returns with save mode Overwrite.
19/09/07 23:33:11 INFO TPCDSTables: Generating table catalog_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_returns with save mode Overwrite.
Generating table inventory in database to s3a://spark-k8s-data/TPCDS-TEST-100G/inventory with save mode Overwrite.
19/09/07 23:33:37 INFO TPCDSTables: Generating table inventory in database to s3a://spark-k8s-data/TPCDS-TEST-100G/inventory with save mode Overwrite.
Generating table store_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_sales with save mode Overwrite.
19/09/07 23:34:15 INFO TPCDSTables: Generating table store_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_sales with save mode Overwrite.
Generating table store_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_returns with save mode Overwrite.
19/09/07 23:37:14 INFO TPCDSTables: Generating table store_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store_returns with save mode Overwrite.
19/09/07 23:37:25 WARN TaskSetManager: Lost task 13.0 in stage 4.0 (TID 413, 192.168.37.252, executor 37): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-100G/store_returns/_temporary/0/_temporary/attempt_20190907233716_0004_m_000013_413/part-00013-e11a2fdb-2085-49c0-b0cc-8f85b639cb4e-c000.snappy.parquet; isDirectory=false; length=16213032; replication=1; blocksize=33554432; modification_time=1567899445000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-100G/store_returns/part-00013-e11a2fdb-2085-49c0-b0cc-8f85b639cb4e-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table web_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_sales with save mode Overwrite.
19/09/07 23:37:37 INFO TPCDSTables: Generating table web_sales in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_sales with save mode Overwrite.
Generating table web_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_returns with save mode Overwrite.
19/09/07 23:38:45 INFO TPCDSTables: Generating table web_returns in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_returns with save mode Overwrite.
19/09/07 23:38:51 WARN TaskSetManager: Lost task 3.0 in stage 6.0 (TID 604, 192.168.10.48, executor 47): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to rename S3AFileStatus{path=s3a://spark-k8s-data/TPCDS-TEST-100G/web_returns/_temporary/0/_temporary/attempt_20190907233846_0006_m_000003_604/part-00003-a8a25ee1-b7e1-433d-a780-ef121dda00f6-c000.snappy.parquet; isDirectory=false; length=5500504; replication=1; blocksize=33554432; modification_time=1567899531000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE to s3a://spark-k8s-data/TPCDS-TEST-100G/web_returns/part-00003-a8a25ee1-b7e1-433d-a780-ef121dda00f6-c000.snappy.parquet
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:560)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	... 10 more

Generating table call_center in database to s3a://spark-k8s-data/TPCDS-TEST-100G/call_center with save mode Overwrite.
19/09/07 23:38:56 INFO TPCDSTables: Generating table call_center in database to s3a://spark-k8s-data/TPCDS-TEST-100G/call_center with save mode Overwrite.
Generating table catalog_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_page with save mode Overwrite.
19/09/07 23:38:58 INFO TPCDSTables: Generating table catalog_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/catalog_page with save mode Overwrite.
Generating table customer in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer with save mode Overwrite.
19/09/07 23:39:00 INFO TPCDSTables: Generating table customer in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer with save mode Overwrite.
Generating table customer_address in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_address with save mode Overwrite.
19/09/07 23:39:06 INFO TPCDSTables: Generating table customer_address in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_address with save mode Overwrite.
Generating table customer_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_demographics with save mode Overwrite.
19/09/07 23:39:11 INFO TPCDSTables: Generating table customer_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/customer_demographics with save mode Overwrite.
Generating table date_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/date_dim with save mode Overwrite.
19/09/07 23:39:16 INFO TPCDSTables: Generating table date_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/date_dim with save mode Overwrite.
Generating table household_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/household_demographics with save mode Overwrite.
19/09/07 23:39:19 INFO TPCDSTables: Generating table household_demographics in database to s3a://spark-k8s-data/TPCDS-TEST-100G/household_demographics with save mode Overwrite.
Generating table income_band in database to s3a://spark-k8s-data/TPCDS-TEST-100G/income_band with save mode Overwrite.
19/09/07 23:39:20 INFO TPCDSTables: Generating table income_band in database to s3a://spark-k8s-data/TPCDS-TEST-100G/income_band with save mode Overwrite.
Generating table item in database to s3a://spark-k8s-data/TPCDS-TEST-100G/item with save mode Overwrite.
19/09/07 23:39:22 INFO TPCDSTables: Generating table item in database to s3a://spark-k8s-data/TPCDS-TEST-100G/item with save mode Overwrite.
Generating table promotion in database to s3a://spark-k8s-data/TPCDS-TEST-100G/promotion with save mode Overwrite.
19/09/07 23:39:28 INFO TPCDSTables: Generating table promotion in database to s3a://spark-k8s-data/TPCDS-TEST-100G/promotion with save mode Overwrite.
Generating table reason in database to s3a://spark-k8s-data/TPCDS-TEST-100G/reason with save mode Overwrite.
19/09/07 23:39:29 INFO TPCDSTables: Generating table reason in database to s3a://spark-k8s-data/TPCDS-TEST-100G/reason with save mode Overwrite.
Generating table ship_mode in database to s3a://spark-k8s-data/TPCDS-TEST-100G/ship_mode with save mode Overwrite.
19/09/07 23:39:31 INFO TPCDSTables: Generating table ship_mode in database to s3a://spark-k8s-data/TPCDS-TEST-100G/ship_mode with save mode Overwrite.
Generating table store in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store with save mode Overwrite.
19/09/07 23:39:33 INFO TPCDSTables: Generating table store in database to s3a://spark-k8s-data/TPCDS-TEST-100G/store with save mode Overwrite.
Generating table time_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/time_dim with save mode Overwrite.
19/09/07 23:39:34 INFO TPCDSTables: Generating table time_dim in database to s3a://spark-k8s-data/TPCDS-TEST-100G/time_dim with save mode Overwrite.
Generating table warehouse in database to s3a://spark-k8s-data/TPCDS-TEST-100G/warehouse with save mode Overwrite.
19/09/07 23:39:37 INFO TPCDSTables: Generating table warehouse in database to s3a://spark-k8s-data/TPCDS-TEST-100G/warehouse with save mode Overwrite.
Generating table web_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_page with save mode Overwrite.
19/09/07 23:39:38 INFO TPCDSTables: Generating table web_page in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_page with save mode Overwrite.
Generating table web_site in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_site with save mode Overwrite.
19/09/07 23:39:40 INFO TPCDSTables: Generating table web_site in database to s3a://spark-k8s-data/TPCDS-TEST-100G/web_site with save mode Overwrite.
Data generated at s3a://spark-k8s-data/TPCDS-TEST-100G
19/09/07 23:39:42 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
