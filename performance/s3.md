1. Rename improvements and consistent view - FileOutputCommitter
2. local dir use hostPath instead or tmpfs memory
3. more disk for spark.local.dir


### Throttled requests from S3 and Dynamo DB (read)
When S3A or Dynamo DB returns a response indicating that requests from the caller are being throttled, an exponential back-off with an initial interval and a maximum number of requests.

```xml
<property>
  <name>fs.s3a.retry.throttle.limit</name>
  <value>${fs.s3a.attempts.maximum}</value>
  <description>
    Number of times to retry any throttled request.
  </description>
</property>

<property>
  <name>fs.s3a.retry.throttle.interval</name>
  <value>1000ms</value>
  <description>
    Interval between retry attempts on throttled requests.
  </description>
</property>
```


Notes

- There is also throttling taking place inside the AWS SDK; this is managed by the value fs.s3a.attempts.maximum.
- Throttling events are tracked in the S3A filesystem metrics and statistics.
- Amazon KMS may thottle a customer based on the total rate of uses of KMS across all user accounts and applications.

Throttling of S3 requests is all too common; it is caused by too many clients trying to access the
same shard of S3 Storage. This generatlly happen if there are too many reads, those being the most common in Hadoop applications. This problem is exacerbated by Hive’s partitioning strategy used when storing data, such as partitioning by year and then month. This results in paths with little or no variation at their start, which ends up in all the data being stored in the same shard(s).

Here are some expensive operations; the more of these taking place against part of an S3 bucket, the more load it experiences. * Many clients trying to list directories or calling getFileStatus on paths (LIST and HEAD requests respectively) * The GET requests issued when reading data. * Random IO used when reading columnar data (ORC, Parquet) means that many more GET requests than a simple one-per-file read. * The number of active writes to that part of the S3 bucket.

A special case is when enough data has been written into part of an S3 bucket that S3 decides to split the data across more than one shard: this is believed to be one by some copy operation which can take some time. While this is under way, S3 clients access data under these paths will be throttled more than usual.

Mitigation strategies

- Use separate buckets for intermediate data/different applications/roles.
- Use significantly different paths for different datasets in the same bucket.
- Increase the value of fs.s3a.retry.throttle.interval to provide longer delays between attempts.
- Reduce the parallelism of the queries. The more tasks trying to access data in parallel, the more load.
- Reduce fs.s3a.threads.max to reduce the amount of parallel operations performed by clients. !. Maybe: increase fs.s3a.readahead.range to increase the minimum amount of data asked for in every GET request, as well as how much data is skipped in the existing stream before aborting it and creating a new stream.
- If the DynamoDB tables used by S3Guard are being throttled, increase the capacity through hadoop s3guard set-capacity (and pay more, obviously).
- KMS: “consult AWS about increating your capacity”.


### How S3A writes data to S3 (write)

The original S3A client implemented file writes by buffering all data to disk as it was written to the OutputStream. Only when the stream’s close() method was called would the upload start.

This made output slow, especially on large uploads, and could even fill up the disk space of small (virtual) disks.

Hadoop 2.7 added the S3AFastOutputStream alternative, which Hadoop 2.8 expanded. It is now considered stable and has replaced the original S3AOutputStream, which is no longer shipped in hadoop.

The “fast” output stream

- Uploads large files as blocks with the size set by fs.s3a.multipart.size. That is: the threshold at which multipart uploads begin and the size of each upload are identical.
- Buffers blocks to disk (default) or in on-heap or off-heap memory.
- Uploads blocks in parallel in background threads.
- Begins uploading blocks as soon as the buffered data exceeds this partition size.
- When buffering data to disk, uses the directory/directories listed in fs.s3a.buffer.dir. The size of data which can be buffered is limited to the available disk space.
- Generates output statistics as metrics on the filesystem, including statistics of active and pending block uploads.
- Has the time to close() set by the amount of remaning data to upload, rather than the total size of the file.

Because it starts uploading while data is still being written, it offers significant benefits when very large amounts of data are generated. The in memory buffering mechanims may also offer speedup when running adjacent to S3 endpoints, as disks are not used for intermediate data storage.



### S3 Committer

Staging committer - Netflix
Magic committer - S3 Guard must be enabled